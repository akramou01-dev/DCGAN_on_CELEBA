{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# DCGANs\n\nDCGAN means a GAN that uses the convolutional and convolutional-transpose layers in the descriminator and generator models respectively  \n1. **Descriminator**: is composed of conv2d, batch normalization layers and uses the LeakyReLU activation with 0.2 as slope. it's input is a 3 x 64 x 64 images and the output is the proba that this image is real (it belongs to the real data distribution) \n2. **Generator** : for the gen, it uses convolutional-transpose and batch normalization layer, with the ReLU activation,the input is a latent vector z and the output is an 3 x 64 x 64  image ( The strided conv-transpose layers allow the latent vector to be transformed into a volume with the same shape as an image)\n\nThe Transformed Conv is almost doing the deconvolutional operation , but it doesn't create exctly the inverse results of the convolutional operation","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"## Import the used packages","metadata":{}},{"cell_type":"code","source":"from __future__ import print_function\nimport argparse\nimport os\nimport random\nimport torch as th\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.optim as optim\nimport torch.utils.data\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nimport torchvision.utils as vutils\nfrom torchvision.utils import make_grid\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\n%matplotlib inline\nfrom IPython.display import HTML","metadata":{"execution":{"iopub.status.busy":"2021-11-20T21:26:27.608410Z","iopub.execute_input":"2021-11-20T21:26:27.608727Z","iopub.status.idle":"2021-11-20T21:26:29.295396Z","shell.execute_reply.started":"2021-11-20T21:26:27.608649Z","shell.execute_reply":"2021-11-20T21:26:29.294577Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"We will define some inputs to have with them in all the notebook ","metadata":{}},{"cell_type":"code","source":"\n\nDATA_PATH = '../input/celeba-dataset/img_align_celeba/img_align_celeba'\n# number of worker threads for loading the data \nN_WORKERS =  4\n# in the DCGAN paper, they used a batch_size of 128 \nbatch_size = 128\n#in this notebook we will use an images of 3 x 64 x 64 (if we desire to change it, we need to changes also in the gen and des architectures)\nimage_size =64 \nnc = 3 # the number of channels (3 for the RGB images)\nnz = 100 # length of the latent vector \nngf = 64 # the depth of the feature maps in  the genegator \nndf =  64 # the depth of the feature maps in the descriminator \n\nEPOCHS = 50\nlr = 0.0002 # the learning rate for the optimizer (we choose 0.0002 relatively to the paper of the DCGAN)\nbeta1 = 0.5  # the hyperparameter for the adam optim\n","metadata":{"execution":{"iopub.status.busy":"2021-11-20T21:26:36.995400Z","iopub.execute_input":"2021-11-20T21:26:36.995723Z","iopub.status.idle":"2021-11-20T21:26:37.001511Z","shell.execute_reply.started":"2021-11-20T21:26:36.995684Z","shell.execute_reply":"2021-11-20T21:26:37.000826Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"We are using the ImageFolder class to extract our data and doing some transformation on it \n\nwe are resizing all the images to the appropriate size that we define earlier and cropping the image on the center, then normalizing it with mean and std","metadata":{}},{"cell_type":"code","source":"dataset = dset.ImageFolder(root=\"../input/celeba-dataset/img_align_celeba\",\n                           transform=transforms.Compose([\n                               transforms.Resize(image_size),\n                               transforms.CenterCrop(image_size),\n                               transforms.ToTensor(),\n                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n                           ]))","metadata":{"execution":{"iopub.status.busy":"2021-11-20T21:26:38.007425Z","iopub.execute_input":"2021-11-20T21:26:38.008181Z","iopub.status.idle":"2021-11-20T21:26:39.971361Z","shell.execute_reply.started":"2021-11-20T21:26:38.008114Z","shell.execute_reply":"2021-11-20T21:26:39.970077Z"},"trusted":true},"execution_count":3,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_692/350109528.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCenterCrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                                \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m                            ]))\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    311\u001b[0m                                           \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m                                           \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m                                           is_valid_file=is_valid_file)\n\u001b[0m\u001b[1;32m    314\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m    144\u001b[0m                                             target_transform=target_transform)\n\u001b[1;32m    145\u001b[0m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mmake_dataset\u001b[0;34m(directory, class_to_idx, extensions, is_valid_file)\u001b[0m\n\u001b[1;32m    190\u001b[0m                 \u001b[0;34m\"The class_to_idx parameter cannot be None.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             )\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_valid_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirectory\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mmake_dataset\u001b[0;34m(directory, class_to_idx, extensions, is_valid_file)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfnames\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwalk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollowlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                 \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/os.py\u001b[0m in \u001b[0;36mwalk\u001b[0;34m(top, topdown, onerror, followlinks)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m                 \u001b[0mis_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m                 \u001b[0;31m# If is_dir() raises an OSError, consider that the entry is not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"# creating the dataloader based on the dataset \ndl = th.utils.data.DataLoader(dataset,batch_size = batch_size , shuffle = True,num_workers = N_WORKERS)","metadata":{"execution":{"iopub.status.busy":"2021-11-20T18:39:16.403557Z","iopub.execute_input":"2021-11-20T18:39:16.403791Z","iopub.status.idle":"2021-11-20T18:39:16.411461Z","shell.execute_reply.started":"2021-11-20T18:39:16.403759Z","shell.execute_reply":"2021-11-20T18:39:16.410616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# setting the default device \ndevice = th.device(\"cuda\" if th.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2021-11-20T21:26:45.989171Z","iopub.execute_input":"2021-11-20T21:26:45.989928Z","iopub.status.idle":"2021-11-20T21:26:46.046390Z","shell.execute_reply.started":"2021-11-20T21:26:45.989888Z","shell.execute_reply":"2021-11-20T21:26:46.045107Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"#creating a helper fonction to show some real images samples \ndef show_batch(dl): \n    for images, _ in dl : \n        fig, ax = plt.subplots(figsize = (16,10))\n        ax.set_xticks([])\n        ax.set_yticks([])\n        ax.imshow(make_grid(images[:20],10).permute(1,2,0))\n        break","metadata":{"execution":{"iopub.status.busy":"2021-11-20T21:26:46.288364Z","iopub.execute_input":"2021-11-20T21:26:46.288870Z","iopub.status.idle":"2021-11-20T21:26:46.295074Z","shell.execute_reply.started":"2021-11-20T21:26:46.288834Z","shell.execute_reply":"2021-11-20T21:26:46.293473Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"#plotting some samples \nshow_batch(dl)","metadata":{"execution":{"iopub.status.busy":"2021-11-20T21:26:46.817886Z","iopub.execute_input":"2021-11-20T21:26:46.818401Z","iopub.status.idle":"2021-11-20T21:26:46.833465Z","shell.execute_reply.started":"2021-11-20T21:26:46.818364Z","shell.execute_reply":"2021-11-20T21:26:46.832330Z"},"trusted":true},"execution_count":6,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_692/694315581.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#plotting some samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mshow_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'dl' is not defined"],"ename":"NameError","evalue":"name 'dl' is not defined","output_type":"error"}]},{"cell_type":"markdown","source":"For the initialization of the model, we are going to use a random weights and bias from the Normale Distribution with mean=0 and std = 0.02 as specified in the DCGAN paper.\nfor this we are creating the **weight_init** fonction that take a initialized model and re-initialize the weights and the bias of the convolutional, convolutional-transpose and batchNormalize layers with from the normale dist.","metadata":{}},{"cell_type":"code","source":"def weight_init(model): \n    class_name = model.__class__.__name__\n    if class_name.find(\"Conv\") !=-1:  # the convolutional and convolutional-transpose layers\n        #nn.init.normal_(tensor) fonction that fils the input tensor with values from the normale distribution \n        nn.init.normal_(model.weight.data,mean=0.0,std=0.02)\n    elif  class_name.find(\"BatchNorm\") !=-1: # the batch normalization layer\n        nn.init.normal_(model.weight.data, 1.0 , 0.02)\n        # nn.init.constant_(tensor , const) fils the input tensor with the constant \"const\"\n        nn.init.constant_(model.bias.data,0)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-20T21:26:53.357770Z","iopub.execute_input":"2021-11-20T21:26:53.358024Z","iopub.status.idle":"2021-11-20T21:26:53.363577Z","shell.execute_reply.started":"2021-11-20T21:26:53.357996Z","shell.execute_reply":"2021-11-20T21:26:53.362885Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Generator\n\nThe generator try to map the latent space vector with the data space (real values), and this by creating a RGB images like the training images (3x64x64).\n\nWe use 2d convolutional transpose layer with 2d batch normalization and ReLU fonction, and the output is fed through a Tanh fonction to return it into input data ranfge [-1,1]\n\n**for the generator we are going to use the ConvTranspose2d layer which can be seen as the inverse of the conv2d layer (but it's not realy the inverse because it doesn't give as the correct inverse)**","metadata":{}},{"cell_type":"code","source":"class Generator(nn.Module): \n    def __init__(self): \n        super(Generator,self).__init__()\n        self.main = nn.Sequential(\n            # we are going to fit the latent vector into a convolutional\n            nn.ConvTranspose2d(nz,ngf*8,4,1,0,bias=False),\n            # ngf * 8 is the ouput of the ConvTranspose layer\n            nn.BatchNorm2d(ngf*8),\n            nn.ReLU(True),\n            # as we have inputs of shape 3 x 64 x 64 so after applying the convTranspose operation we will have\n            # output of shape (ngf*8) x 4 x 4  \n            \n            nn.ConvTranspose2d(ngf * 8 , ngf * 4 , 4 , stride=2,padding=1,bias=False),\n            #as we set tha stride =2 and we are appying the ConvTranspose operation \n            # so we will multiply the output shape by 2,contrary to the conv2d operation which devide by the stride\n            nn.BatchNorm2d(ngf*4),\n            nn.ReLU(True),\n            # output shape will be  (ngf*4) x 8 x 8\n\n            nn.ConvTranspose2d(ngf*4 , ngf*2, 4 , 2, 1,bias=False), # by applying a stride of 2 so the output shape will multiplyed by 2 \n            nn.BatchNorm2d(ngf*2),\n            nn.ReLU(True),\n            # output shape will be  (ngf*2) x 16 x 16\n            \n            # we will continue our convTranspose operation until we obtain the shape of our images (3x64x64)\n            nn.ConvTranspose2d(ngf*2 , ngf, 4 , 2, 1,bias=False),\n            nn.BatchNorm2d(ngf),\n            nn.ReLU(True),\n            # output shape will be  ngf x 32 x 32\n            \n            nn.ConvTranspose2d(ngf,nc, 4 , 2 , 1,bias=False),\n            nn.Tanh(),\n            # output shape will be  nc x 64 x 64            \n        )\n    def forward(self,x):\n        return self.main(x)\n    ","metadata":{"execution":{"iopub.status.busy":"2021-11-20T20:22:09.78867Z","iopub.execute_input":"2021-11-20T20:22:09.789066Z","iopub.status.idle":"2021-11-20T20:22:09.801089Z","shell.execute_reply.started":"2021-11-20T20:22:09.789036Z","shell.execute_reply":"2021-11-20T20:22:09.8Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here that we build the Generator network, we will fit it to the weight init fonction so that we initialize the weights from the normal distribution","metadata":{}},{"cell_type":"code","source":"Gen =Generator()\nprint(Gen.main[0].weight.data[0])","metadata":{"execution":{"iopub.status.busy":"2021-11-20T20:22:50.128418Z","iopub.execute_input":"2021-11-20T20:22:50.128951Z","iopub.status.idle":"2021-11-20T20:22:50.162522Z","shell.execute_reply.started":"2021-11-20T20:22:50.128913Z","shell.execute_reply":"2021-11-20T20:22:50.161801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Gen = Generator().to(device)\n\n# Apply the weights_init function to randomly initialize all weights to mean=0, stdev=0.02\n\nGen.apply(weight_init)\n\nprint(Gen)","metadata":{"execution":{"iopub.status.busy":"2021-11-20T20:23:16.587659Z","iopub.execute_input":"2021-11-20T20:23:16.588219Z","iopub.status.idle":"2021-11-20T20:23:16.625037Z","shell.execute_reply.started":"2021-11-20T20:23:16.588183Z","shell.execute_reply":"2021-11-20T20:23:16.624349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Gen.main[0].weight.data[0]","metadata":{"execution":{"iopub.status.busy":"2021-11-20T20:23:30.957146Z","iopub.execute_input":"2021-11-20T20:23:30.957918Z","iopub.status.idle":"2021-11-20T20:23:31.004436Z","shell.execute_reply.started":"2021-11-20T20:23:30.957873Z","shell.execute_reply":"2021-11-20T20:23:31.003556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Descriminator\n\nThe descriminator will be a classifier that output the proba that the image is real, for this we are going to use a series of conv2d, batch normalization layers with the Leaky Relu activation fonction with slope of 0.2,and also a sigmoid fonction to convert the outputs to probabilities, we can extend this arcitecture with more layers,\n\nin the DCGAN paper we mentioned that it's a good practise to use strided conv2d layers instead of using the maxpooling and that because it lets the network to learn it's own pooling fonction.","metadata":{}},{"cell_type":"code","source":"class Descriminator(nn.Module):\n    def __init__(self):\n        super(Descriminator,self).__init__()\n        self.main = nn.Sequential(\n        # the input will be an image of : nc x 64 x 64 image\n            nn.Conv2d(nc,ndf,4,2,1,bias=False),\n            nn.BatchNorm2d(ndf),\n            nn.LeakyReLU(0.2), # slop of 0.2 is recommended by the comunity \n            #as we are using the conv2d layers with stride of 2, so the shapes of each channels will be devided by 2 \n            # output shape :  ndf x 32 x 32\n            \n            nn.Conv2d(ndf, ndf*2,4,2,1,bias=False),\n            nn.BatchNorm2d(ndf*2),\n            nn.LeakyReLU(0.2),\n            #output shape : (ndf*2) x 16 x 16\n\n            nn.Conv2d(ndf*2,ndf*4,4,2,1,bias=False),\n            nn.BatchNorm2d(ndf*4),\n            nn.LeakyReLU(0.2),\n            # output size : (ndf*4) x 8 x 8\n            nn.Conv2d(ndf*4,ndf*8,4,2,1,bias=False),\n            nn.BatchNorm2d(ndf*8),\n            nn.LeakyReLU(0.2),\n            #output size : ndf*8 x 4 x 4\n            nn.Conv2d(ndf*8,1,4,2,1,bias=False),\n            nn.Sigmoid(),\n        )\n    def forward(self,input):\n        return self.main(input)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-20T21:35:42.076190Z","iopub.execute_input":"2021-11-20T21:35:42.076730Z","iopub.status.idle":"2021-11-20T21:35:42.087776Z","shell.execute_reply.started":"2021-11-20T21:35:42.076691Z","shell.execute_reply":"2021-11-20T21:35:42.087011Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"We are going to do the same initialization for the Descriminator ","metadata":{}},{"cell_type":"code","source":"Des = Descriminator().to(device)\n\nDes.apply(weight_init)\n\nprint(Des)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-20T21:37:12.101356Z","iopub.execute_input":"2021-11-20T21:37:12.101911Z","iopub.status.idle":"2021-11-20T21:37:12.135570Z","shell.execute_reply.started":"2021-11-20T21:37:12.101872Z","shell.execute_reply":"2021-11-20T21:37:12.134801Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Descriminator(\n  (main): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): LeakyReLU(negative_slope=0.2)\n    (3): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (5): LeakyReLU(negative_slope=0.2)\n    (6): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (8): LeakyReLU(negative_slope=0.2)\n    (9): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (10): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (11): LeakyReLU(negative_slope=0.2)\n    (12): Conv2d(512, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (13): Sigmoid()\n  )\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}